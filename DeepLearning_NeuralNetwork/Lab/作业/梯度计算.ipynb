{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical dL_dU:\n",
      " [[ 0.          0.38865328  0.          0.        ]\n",
      " [-0.         -0.3886532  -0.         -0.        ]]\n",
      "Numerical dL_dU:\n",
      " [[ 0.         0.3874302  0.         0.       ]\n",
      " [ 0.        -0.3874302  0.         0.       ]]\n",
      "Analytical dL_db2:\n",
      " [ 0.25237226 -0.2523722 ]\n",
      "Numerical dL_db2:\n",
      " [ 0.25480986 -0.2503395 ]\n",
      "Analytical dL_dW:\n",
      " [[ 0.         -0.          0.        ]\n",
      " [-0.11356751  0.22713502 -0.18170802]\n",
      " [-0.          0.         -0.        ]\n",
      " [ 0.         -0.          0.        ]]\n",
      "Numerical dL_dW:\n",
      " [[ 0.          0.          0.        ]\n",
      " [-0.11175871  0.22798777 -0.18328428]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "Analytical dL_db1:\n",
      " [ 0.         -0.22713502 -0.          0.        ]\n",
      "Numerical dL_db1:\n",
      " [ 0.         -0.22798777  0.          0.        ]\n",
      "Analytical dL_dx:\n",
      " [-0.04542701  0.22713502 -0.18170802]\n",
      "Numerical dL_dx:\n",
      " [-0.04917383  0.22798777 -0.18328428]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Define the network parameters\n",
    "# --- 定义确定的网络参数，便于调试和验证 ---\n",
    "D, H, C = 3, 4, 2  # 输入维度(D=3), 隐藏层大小(H=4), 输出类别数(C=2)\n",
    "\n",
    "# 手动设定固定参数（非随机）\n",
    "W = np.array([\n",
    "    [1.0, 0.5, -0.3],   # 隐藏层第1个神经元的权重\n",
    "    [0.2, -1.0, 0.8],   # 隐藏层第2个神经元的权重\n",
    "    [0.4, 0.7, -0.9],   # 隐藏层第3个神经元的权重\n",
    "    [-0.6, 0.1, 0.5]    # 隐藏层第4个神经元的权重\n",
    "], dtype=np.float32)     # 形状 (4,3)\n",
    "\n",
    "b1 = np.array([0.1, -0.2, 0.3, -0.4], dtype=np.float32)  # 隐藏层偏置 (4,)\n",
    "\n",
    "U = np.array([\n",
    "    [0.5, -0.3, 0.2, 0.1],   # 输出层第1个类别的权重\n",
    "    [-0.4, 0.6, 0.7, -0.8]   # 输出层第2个类别的权重\n",
    "], dtype=np.float32)          # 形状 (2,4)\n",
    "\n",
    "b2 = np.array([0.2, -0.1], dtype=np.float32)  # 输出层偏置 (2,)\n",
    "\n",
    "x = np.array([0.5, -1.0, 0.8], dtype=np.float32)  # 输入向量 (3,)\n",
    "\n",
    "y_true = np.array([0, 1], dtype=np.float32)       # 真实标签（第二类）\n",
    "\n",
    "# Define the forward pass\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def softmax(theta):\n",
    "    exp_theta = np.exp(theta - np.max(theta))\n",
    "    return exp_theta / np.sum(exp_theta)\n",
    "\n",
    "def cross_entropy(y, y_true):\n",
    "    return -np.sum(y_true * np.log(y))\n",
    "\n",
    "def forward(x, W, b1, U, b2):\n",
    "    z = W @ x + b1\n",
    "    h = relu(z)\n",
    "    theta = U @ h + b2\n",
    "    y = softmax(theta)\n",
    "    loss = cross_entropy(y, y_true)\n",
    "    return loss, z, h, theta, y\n",
    "\n",
    "# Compute analytical gradients\n",
    "loss, z, h, theta, y = forward(x, W, b1, U, b2)\n",
    "dL_dtheta = y - y_true\n",
    "dL_dU = np.outer(dL_dtheta, h)\n",
    "dL_db2 = dL_dtheta\n",
    "dL_dh = U.T @ dL_dtheta\n",
    "dL_dz = dL_dh * (z > 0)\n",
    "dL_dW = np.outer(dL_dz, x)\n",
    "dL_db1 = dL_dz\n",
    "dL_dx = W.T @ dL_dz\n",
    "\n",
    "# Numerical gradient approximation\n",
    "h_numerical = 1e-5\n",
    "\n",
    "def numerical_gradient(f, param):\n",
    "    param_plus = param + h_numerical\n",
    "    param_minus = param - h_numerical\n",
    "    return (f(param_plus) - f(param_minus)) / (2 * h_numerical)\n",
    "\n",
    "# Compute numerical gradients\n",
    "numerical_dL_dU = np.zeros_like(U)\n",
    "for i in range(U.shape[0]):\n",
    "    for j in range(U.shape[1]):\n",
    "        def f_U(u):\n",
    "            U_temp = U.copy()\n",
    "            U_temp[i, j] = u\n",
    "            loss, _, _, _, _ = forward(x, W, b1, U_temp, b2)\n",
    "            return loss\n",
    "        numerical_dL_dU[i, j] = numerical_gradient(f_U, U[i, j])\n",
    "\n",
    "numerical_dL_db2 = np.zeros_like(b2)\n",
    "for i in range(b2.shape[0]):\n",
    "    def f_b2(b):\n",
    "        b2_temp = b2.copy()\n",
    "        b2_temp[i] = b\n",
    "        loss, _, _, _, _ = forward(x, W, b1, U, b2_temp)\n",
    "        return loss\n",
    "    numerical_dL_db2[i] = numerical_gradient(f_b2, b2[i])\n",
    "\n",
    "numerical_dL_dW = np.zeros_like(W)\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(W.shape[1]):\n",
    "        def f_W(w):\n",
    "            W_temp = W.copy()\n",
    "            W_temp[i, j] = w\n",
    "            loss, _, _, _, _ = forward(x, W_temp, b1, U, b2)\n",
    "            return loss\n",
    "        numerical_dL_dW[i, j] = numerical_gradient(f_W, W[i, j])\n",
    "\n",
    "numerical_dL_db1 = np.zeros_like(b1)\n",
    "for i in range(b1.shape[0]):\n",
    "    def f_b1(b):\n",
    "        b1_temp = b1.copy()\n",
    "        b1_temp[i] = b\n",
    "        loss, _, _, _, _ = forward(x, W, b1_temp, U, b2)\n",
    "        return loss\n",
    "    numerical_dL_db1[i] = numerical_gradient(f_b1, b1[i])\n",
    "\n",
    "numerical_dL_dx = np.zeros_like(x)\n",
    "for i in range(x.shape[0]):\n",
    "    def f_x(x_val):\n",
    "        x_temp = x.copy()\n",
    "        x_temp[i] = x_val\n",
    "        loss, _, _, _, _ = forward(x_temp, W, b1, U, b2)\n",
    "        return loss\n",
    "    numerical_dL_dx[i] = numerical_gradient(f_x, x[i])\n",
    "\n",
    "# Compare analytical and numerical gradients\n",
    "print(\"Analytical dL_dU:\\n\", dL_dU)\n",
    "print(\"Numerical dL_dU:\\n\", numerical_dL_dU)\n",
    "print(\"Analytical dL_db2:\\n\", dL_db2)\n",
    "print(\"Numerical dL_db2:\\n\", numerical_dL_db2)\n",
    "print(\"Analytical dL_dW:\\n\", dL_dW)\n",
    "print(\"Numerical dL_dW:\\n\", numerical_dL_dW)\n",
    "print(\"Analytical dL_db1:\\n\", dL_db1)\n",
    "print(\"Numerical dL_db1:\\n\", numerical_dL_db1)\n",
    "print(\"Analytical dL_dx:\\n\", dL_dx)\n",
    "print(\"Numerical dL_dx:\\n\", numerical_dL_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch dL_dU:\n",
      " tensor([[ 0.0000,  0.3887,  0.0000,  0.0000],\n",
      "        [-0.0000, -0.3887, -0.0000, -0.0000]])\n",
      "PyTorch dL_db2:\n",
      " tensor([ 0.2524, -0.2524])\n",
      "PyTorch dL_dW:\n",
      " tensor([[ 0.0000, -0.0000,  0.0000],\n",
      "        [-0.1136,  0.2271, -0.1817],\n",
      "        [ 0.0000, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000]])\n",
      "PyTorch dL_db1:\n",
      " tensor([ 0.0000, -0.2271,  0.0000,  0.0000])\n",
      "PyTorch dL_dx:\n",
      " tensor([-0.0454,  0.2271, -0.1817])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the network parameters\n",
    "W_torch = torch.tensor(W, requires_grad=True)\n",
    "b1_torch = torch.tensor(b1, requires_grad=True)\n",
    "U_torch = torch.tensor(U, requires_grad=True)\n",
    "b2_torch = torch.tensor(b2, requires_grad=True)\n",
    "x_torch = torch.tensor(x, requires_grad=True)\n",
    "y_true_torch = torch.tensor(y_true)\n",
    "\n",
    "# Forward pass\n",
    "z_torch = W_torch @ x_torch + b1_torch\n",
    "h_torch = torch.relu(z_torch)\n",
    "theta_torch = U_torch @ h_torch + b2_torch\n",
    "y_torch = torch.softmax(theta_torch, dim=0)\n",
    "loss_torch = -torch.sum(y_true_torch * torch.log(y_torch))\n",
    "\n",
    "# Backward pass\n",
    "loss_torch.backward()\n",
    "\n",
    "# Compare gradients\n",
    "print(\"PyTorch dL_dU:\\n\", U_torch.grad)\n",
    "print(\"PyTorch dL_db2:\\n\", b2_torch.grad)\n",
    "print(\"PyTorch dL_dW:\\n\", W_torch.grad)\n",
    "print(\"PyTorch dL_db1:\\n\", b1_torch.grad)\n",
    "print(\"PyTorch dL_dx:\\n\", x_torch.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical dL_dU:\n",
      " [[ 0.          0.38865328  0.          0.        ]\n",
      " [-0.         -0.3886532  -0.         -0.        ]]\n",
      "Numerical dL_dU:\n",
      " [[ 0.         0.3874302  0.         0.       ]\n",
      " [ 0.        -0.3874302  0.         0.       ]]\n",
      "Analytical dL_db2:\n",
      " [ 0.25237226 -0.2523722 ]\n",
      "Numerical dL_db2:\n",
      " [ 0.25480986 -0.2503395 ]\n",
      "Analytical dL_dW:\n",
      " [[ 0.         -0.          0.        ]\n",
      " [-0.11356751  0.22713502 -0.18170802]\n",
      " [-0.          0.         -0.        ]\n",
      " [ 0.         -0.          0.        ]]\n",
      "Numerical dL_dW:\n",
      " [[ 0.          0.          0.        ]\n",
      " [-0.11175871  0.22798777 -0.18328428]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "Analytical dL_db1:\n",
      " [ 0.         -0.22713502 -0.          0.        ]\n",
      "Numerical dL_db1:\n",
      " [ 0.         -0.22798777  0.          0.        ]\n",
      "Analytical dL_dx:\n",
      " [-0.04542701  0.22713502 -0.18170802]\n",
      "Numerical dL_dx:\n",
      " [-0.04917383  0.22798777 -0.18328428]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# --- 定义确定的网络参数，便于调试和验证 ---\n",
    "D, H, C = 3, 4, 2  # 输入维度(D=3), 隐藏层大小(H=4), 输出类别数(C=2)\n",
    "\n",
    "# 手动设定固定参数（非随机）\n",
    "W = np.array([\n",
    "    [1.0, 0.5, -0.3],   # 隐藏层第1个神经元的权重\n",
    "    [0.2, -1.0, 0.8],   # 隐藏层第2个神经元的权重\n",
    "    [0.4, 0.7, -0.9],   # 隐藏层第3个神经元的权重\n",
    "    [-0.6, 0.1, 0.5]    # 隐藏层第4个神经元的权重\n",
    "], dtype=np.float32)     # 形状 (4,3)\n",
    "\n",
    "b1 = np.array([0.1, -0.2, 0.3, -0.4], dtype=np.float32)  # 隐藏层偏置 (4,)\n",
    "\n",
    "U = np.array([\n",
    "    [0.5, -0.3, 0.2, 0.1],   # 输出层第1个类别的权重\n",
    "    [-0.4, 0.6, 0.7, -0.8]   # 输出层第2个类别的权重\n",
    "], dtype=np.float32)          # 形状 (2,4)\n",
    "\n",
    "b2 = np.array([0.2, -0.1], dtype=np.float32)  # 输出层偏置 (2,)\n",
    "\n",
    "x = np.array([0.5, -1.0, 0.8], dtype=np.float32)  # 输入向量 (3,)\n",
    "\n",
    "y_true = np.array([0, 1], dtype=np.float32)       # 真实标签（第二类）\n",
    "\n",
    "# --- 前向传播函数（保持不变）---\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def softmax(theta):\n",
    "    exp_theta = np.exp(theta - np.max(theta))\n",
    "    return exp_theta / np.sum(exp_theta)\n",
    "\n",
    "def cross_entropy(y, y_true):\n",
    "    return -np.sum(y_true * np.log(y + 1e-12))  # 添加微小值防止log(0)\n",
    "\n",
    "def forward(x, W, b1, U, b2):\n",
    "    z = W @ x + b1\n",
    "    h = relu(z)\n",
    "    theta = U @ h + b2\n",
    "    y = softmax(theta)\n",
    "    loss = cross_entropy(y, y_true)\n",
    "    return loss, z, h, theta, y\n",
    "\n",
    "# --- 计算解析梯度 ---\n",
    "loss, z, h, theta, y = forward(x, W, b1, U, b2)\n",
    "\n",
    "# 梯度计算步骤\n",
    "dL_dtheta = y - y_true  # (2,)\n",
    "dL_dU = np.outer(dL_dtheta, h)  # (2,4)\n",
    "dL_db2 = dL_dtheta.copy()        # (2,)\n",
    "dL_dh = U.T @ dL_dtheta         # (4,)\n",
    "dL_dz = dL_dh * (z > 0)         # (4,)\n",
    "dL_dW = np.outer(dL_dz, x)      # (4,3)\n",
    "dL_db1 = dL_dz.copy()           # (4,)\n",
    "dL_dx = W.T @ dL_dz            # (3,)\n",
    "\n",
    "# --- 数值梯度计算函数（修正后）---\n",
    "h_numerical = 1e-5\n",
    "\n",
    "def numerical_gradient(f, param):\n",
    "    param_plus = param + h_numerical\n",
    "    param_minus = param - h_numerical\n",
    "    return (f(param_plus) - f(param_minus)) / (2 * h_numerical)\n",
    "\n",
    "# 计算数值梯度\n",
    "def compute_numerical_gradients():\n",
    "    numerical = {}\n",
    "    \n",
    "    # U的梯度\n",
    "    numerical_dL_dU = np.zeros_like(U)\n",
    "    for i in range(U.shape[0]):\n",
    "        for j in range(U.shape[1]):\n",
    "            def f_U(u_val):\n",
    "                U_temp = U.copy()\n",
    "                U_temp[i,j] = u_val\n",
    "                loss,_,_,_,_ = forward(x, W, b1, U_temp, b2)\n",
    "                return loss\n",
    "            numerical_dL_dU[i,j] = numerical_gradient(f_U, U[i,j])\n",
    "    numerical['U'] = numerical_dL_dU\n",
    "\n",
    "    # b2的梯度\n",
    "    numerical_dL_db2 = np.zeros_like(b2)\n",
    "    for i in range(b2.shape[0]):\n",
    "        def f_b2(b_val):\n",
    "            b2_temp = b2.copy()\n",
    "            b2_temp[i] = b_val\n",
    "            loss,_,_,_,_ = forward(x, W, b1, U, b2_temp)\n",
    "            return loss\n",
    "        numerical_dL_db2[i] = numerical_gradient(f_b2, b2[i])\n",
    "    numerical['b2'] = numerical_dL_db2\n",
    "\n",
    "    # W的梯度（类似U）\n",
    "    numerical_dL_dW = np.zeros_like(W)\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[1]):\n",
    "            def f_W(w_val):\n",
    "                W_temp = W.copy()\n",
    "                W_temp[i,j] = w_val\n",
    "                loss,_,_,_,_ = forward(x, W_temp, b1, U, b2)\n",
    "                return loss\n",
    "            numerical_dL_dW[i,j] = numerical_gradient(f_W, W[i,j])\n",
    "    numerical['W'] = numerical_dL_dW\n",
    "\n",
    "    # b1的梯度（类似b2）\n",
    "    numerical_dL_db1 = np.zeros_like(b1)\n",
    "    for i in range(b1.shape[0]):\n",
    "        def f_b1(b_val):\n",
    "            b1_temp = b1.copy()\n",
    "            b1_temp[i] = b_val\n",
    "            loss,_,_,_,_ = forward(x, W, b1_temp, U, b2)\n",
    "            return loss\n",
    "        numerical_dL_db1[i] = numerical_gradient(f_b1, b1[i])\n",
    "    numerical['b1'] = numerical_dL_db1\n",
    "\n",
    "    # x的梯度\n",
    "    numerical_dL_dx = np.zeros_like(x)\n",
    "    for i in range(x.shape[0]):\n",
    "        def f_x(x_val):\n",
    "            x_temp = x.copy()\n",
    "            x_temp[i] = x_val\n",
    "            loss,_,_,_,_ = forward(x_temp, W, b1, U, b2)\n",
    "            return loss\n",
    "        numerical_dL_dx[i] = numerical_gradient(f_x, x[i])\n",
    "    numerical['x'] = numerical_dL_dx\n",
    "    \n",
    "    return numerical\n",
    "\n",
    "numerical = compute_numerical_gradients()\n",
    "\n",
    "# --- 结果对比 ---\n",
    "print(\"Analytical dL_dU:\\n\", dL_dU)\n",
    "print(\"Numerical dL_dU:\\n\", numerical_dL_dU)\n",
    "print(\"Analytical dL_db2:\\n\", dL_db2)\n",
    "print(\"Numerical dL_db2:\\n\", numerical_dL_db2)\n",
    "print(\"Analytical dL_dW:\\n\", dL_dW)\n",
    "print(\"Numerical dL_dW:\\n\", numerical_dL_dW)\n",
    "print(\"Analytical dL_db1:\\n\", dL_db1)\n",
    "print(\"Numerical dL_db1:\\n\", numerical_dL_db1)\n",
    "print(\"Analytical dL_dx:\\n\", dL_dx)\n",
    "print(\"Numerical dL_dx:\\n\", numerical_dL_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch dL_dU:\n",
      " tensor([[ 0.0000,  0.3887,  0.0000,  0.0000],\n",
      "        [-0.0000, -0.3887, -0.0000, -0.0000]])\n",
      "PyTorch dL_db2:\n",
      " tensor([ 0.2524, -0.2524])\n",
      "PyTorch dL_dW:\n",
      " tensor([[ 0.0000, -0.0000,  0.0000],\n",
      "        [-0.1136,  0.2271, -0.1817],\n",
      "        [ 0.0000, -0.0000,  0.0000],\n",
      "        [ 0.0000, -0.0000,  0.0000]])\n",
      "PyTorch dL_db1:\n",
      " tensor([ 0.0000, -0.2271,  0.0000,  0.0000])\n",
      "PyTorch dL_dx:\n",
      " tensor([-0.0454,  0.2271, -0.1817])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the network parameters\n",
    "W_torch = torch.tensor(W, requires_grad=True)\n",
    "b1_torch = torch.tensor(b1, requires_grad=True)\n",
    "U_torch = torch.tensor(U, requires_grad=True)\n",
    "b2_torch = torch.tensor(b2, requires_grad=True)\n",
    "x_torch = torch.tensor(x, requires_grad=True)\n",
    "y_true_torch = torch.tensor(y_true)\n",
    "\n",
    "# Forward pass\n",
    "z_torch = W_torch @ x_torch + b1_torch\n",
    "h_torch = torch.relu(z_torch)\n",
    "theta_torch = U_torch @ h_torch + b2_torch\n",
    "y_torch = torch.softmax(theta_torch, dim=0)\n",
    "loss_torch = -torch.sum(y_true_torch * torch.log(y_torch))\n",
    "\n",
    "# Backward pass\n",
    "loss_torch.backward()\n",
    "\n",
    "# Compare gradients\n",
    "print(\"PyTorch dL_dU:\\n\", U_torch.grad)\n",
    "print(\"PyTorch dL_db2:\\n\", b2_torch.grad)\n",
    "print(\"PyTorch dL_dW:\\n\", W_torch.grad)\n",
    "print(\"PyTorch dL_db1:\\n\", b1_torch.grad)\n",
    "print(\"PyTorch dL_dx:\\n\", x_torch.grad)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
